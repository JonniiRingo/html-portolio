<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <title>Research Paper: Parametric Intersecting Surfaces in Transformer Architectures</title>

  <style>
    .figure {
      display: flex;
      justify-content: center;
    }
    .figure img {
      width: 75%;
      height: auto;
    }
  </style>

</head>

<body>

  <div class="container my-5">
    <!-- Title Page -->
    <section id="title-page" class="text-center mb-5">
      <h1 class="display-4">Exploring Parametric Intersecting Surfaces in Transformer Architectures</h1>
      <h2>A Novel Approach for Multi-Dimensional Contextual Embedding</h2>
      <p class="lead">Jonathan Samuels</p>
      <p class="lead">El Camino College</p>
      <p class="lead">November 2024</p>
    </section>

    <!-- Abstract -->
    <section id="abstract" class="mb-5">
      <h2 class="h3">Abstract</h2>
      <p>
        This paper presents a novel approach to tokenization and embedding in language models by utilizing parametric intersecting surfaces within transformer architectures. By replacing traditional vector fields with intersecting parametric surfaces, this model captures higher-dimensional contextual interactions, offering a more dynamic approach to embedding complex semantic relationships. A mathematical foundation and experimental results are provided to demonstrate the model's increased accuracy and potential benefits for NLP applications.
      </p>
    </section>

    <!-- Table of Contents -->
    <section id="table-of-contents" class="mb-5">
      <h2 class="h3">Table of Contents</h2>
      <ul>
        <li><a href="#introduction">1. Introduction</a></li>
        <li><a href="#framework">2. Theoretical Framework</a></li>
        <li><a href="#methods">3. Methods</a></li>
        <li><a href="#results">4. Results</a></li>
        <li><a href="#implications">5. Implications</a></li>
        <li><a href="#conclusion">6. Conclusion</a></li>
        <li><a href="#references">7. References</a></li>
      </ul>
    </section>

    <!-- Introduction -->
    <section id="introduction" class="mb-5">
      <h2 class="h3">1. Introduction</h2>
      <p>
        Language models have traditionally relied on vector-based embeddings for capturing semantic and syntactic relationships. This paper explores an innovative departure from this paradigm, proposing the use of parametric intersecting surfaces as the foundational structure for embeddings within transformer architectures. We hypothesize that this approach enhances the model's ability to capture complex, multidimensional relationships by treating each token embedding as a surface intersection in a multi-dimensional space.
      </p>
    </section>

    <!-- Theoretical Framework -->
    <section id="framework" class="mb-5">
      <h2 class="h3">2. Theoretical Framework</h2>
      <h5>Parametric Surface Embedding</h5>
      <p>
        The proposed model uses a parametric approach, where each token is represented as a point on a surface in 3D space. The relationship between tokens can then be visualized as intersections between these surfaces, defined by the equation:
      </p>
      <p>
        \[
        S(u, v) = (f_x(u, v), f_y(u, v), f_z(u, v))
        \]
      </p>
      <p>
        where \( S(u, v) \) is a parametric surface defined by parameters \( u \) and \( v \). Here, each function \( f_x, f_y, f_z \) governs the spatial orientation and contextual properties of the token. Intersections between these surfaces represent contextual overlap, giving rise to multi-dimensional embedding spaces.
      </p>

      <h5>Mathematical Foundation</h5>
      <p>
        By using tensor products to capture dependencies between surface intersections, we establish higher-order embeddings. The embedding vector can be represented by:
      </p>
      <p>
        \[
        \vec{E} = \sum_{i,j,k} \vec{e_i}(u, v) \times \vec{e_j}(u, v) \times \vec{e_k}(u, v)
        \]
      </p>
      <p>
        where \( \vec{E} \) is the embedding, and \( \vec{e_i}, \vec{e_j}, \vec{e_k} \) are basis vectors defined along intersecting surfaces.
      </p>
    </section>

    <!-- Methods -->
    <section id="methods" class="mb-5">
      <h2 class="h3">3. Methods</h2>
      <p>
        The 3D parametric embedding layer was implemented using PyTorch, adapting 3D convolutions to accommodate surface-based embeddings. The training corpus was processed into parametric surfaces, with each token represented by a surface in 3D space. Key parameters included:
      </p>
      <ul>
        <li><b>Surface Complexity:</b> Defined by the degree of parametric functions \( f_x, f_y, f_z \).</li>
        <li><b>Intersection Threshold:</b> Adjusted to control the sensitivity of context-dependent intersections.</li>
      </ul>
      <p>
        Through these parameters, the model dynamically adjusted to capture overlapping contexts, evaluated against a baseline using standard 2D embeddings.
      </p>
    </section>

    <!-- Results -->
    <section id="results" class="mb-5">
      <h2 class="h3">4. Results</h2>
      <p>
        Initial experiments demonstrate that the parametric surface embedding significantly outperforms traditional vector embeddings in capturing nuanced semantic relationships. The model's multi-dimensional representation showed a reduction in ambiguity for polysemous words and achieved lower error rates on syntactic benchmarks.
      </p>
      <div class="figure">
        <img src="images/TPISE vs GPT-3.png" alt="Diagram of Parametric Intersecting Surfaces for Token Embedding">
      </div>
    </section>

    <!-- Implications -->
    <section id="implications" class="mb-5">
      <h2 class="h3">5. Implications</h2>
      <p>
        Parametric intersecting surfaces offer an alternative to vector fields by providing a continuous, surface-based approach to contextual embeddings. This innovation has implications for improving model interpretability and enhancing NLP applications, especially in fields requiring spatial awareness, such as multilingual NLP and semantic disambiguation. Future studies could optimize the choice of parametric functions and further explore the implications of surface geometry on embedding fidelity.
      </p>
    </section>

    <!-- Conclusion -->
    <section id="conclusion" class="mb-5">
      <h2 class="h3">6. Conclusion</h2>
      <p>
        By utilizing parametric intersecting surfaces, this study introduces a new embedding approach that enhances the model's ability to represent multi-dimensional contextual information. Future research will focus on refining the interaction mechanics of these surfaces and exploring additional real-world applications for this transformative model architecture.
      </p>
    </section>

    <!-- References -->
    <section id="references" class="mb-5">
      <h2 class="h3">7. References</h2>
      <p>
        [1] Vaswani, A., et al. “Attention Is All You Need.” Advances in Neural Information Processing Systems, 2017.<br>
        [2] Devlin, J., et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” NAACL, 2019.
      </p>
    </section>
  </div>

</body>

</html>
